{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNAjzvsRMpbqtyzoiHf6BFs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zucAG4L2zxhY","executionInfo":{"status":"ok","timestamp":1707330209714,"user_tz":-330,"elapsed":6585,"user":{"displayName":"Harsh Vardhan","userId":"09023081301295531991"}},"outputId":"d70bbf18-08a6-452e-9400-c36a1049d875"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"]}],"source":["!pip install nltk"]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hr9AknmSz2al","executionInfo":{"status":"ok","timestamp":1707330729654,"user_tz":-330,"elapsed":743,"user":{"displayName":"Harsh Vardhan","userId":"09023081301295531991"}},"outputId":"11192f0c-9593-41e9-ed01-44c7ce8cbcab"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["\n","corpus = \"\"\"Hello welcome to NLP's first session. This notebook is overview of Tookenization, have patience you will excel it.\n","\"\"\""],"metadata":{"id":"AcPSsGhIz72Y","executionInfo":{"status":"ok","timestamp":1707331609153,"user_tz":-330,"elapsed":5,"user":{"displayName":"Harsh Vardhan","userId":"09023081301295531991"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["from nltk.tokenize import sent_tokenize\n"],"metadata":{"id":"gtJulqAz0sYW","executionInfo":{"status":"ok","timestamp":1707331613484,"user_tz":-330,"elapsed":13,"user":{"displayName":"Harsh Vardhan","userId":"09023081301295531991"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["#store in a list called documents\n","documents = sent_tokenize(corpus)"],"metadata":{"id":"kl3Zw--u03il","executionInfo":{"status":"ok","timestamp":1707331614185,"user_tz":-330,"elapsed":9,"user":{"displayName":"Harsh Vardhan","userId":"09023081301295531991"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["type(documents)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NyCQuUz9075o","executionInfo":{"status":"ok","timestamp":1707331614186,"user_tz":-330,"elapsed":9,"user":{"displayName":"Harsh Vardhan","userId":"09023081301295531991"}},"outputId":"5a681e5e-5332-4d16-94a2-f4000f25ccdb"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["list"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["for sentence in documents:\n","  print(sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pHrK4cpn4DD5","executionInfo":{"status":"ok","timestamp":1707331614742,"user_tz":-330,"elapsed":3,"user":{"displayName":"Harsh Vardhan","userId":"09023081301295531991"}},"outputId":"c7e33aac-dab6-418d-b97b-d0068e1e8ee0"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello welcome to NLP's first session.\n","This notebook is overview of Tookenization, have patience you will excel it.\n"]}]},{"cell_type":"code","source":["#again tokenization converting sentence to words"],"metadata":{"id":"Q5IIsWG14NB5","executionInfo":{"status":"ok","timestamp":1707331614743,"user_tz":-330,"elapsed":3,"user":{"displayName":"Harsh Vardhan","userId":"09023081301295531991"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","word_tokenize(corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZKhH26Wt4VcW","executionInfo":{"status":"ok","timestamp":1707331615345,"user_tz":-330,"elapsed":6,"user":{"displayName":"Harsh Vardhan","userId":"09023081301295531991"}},"outputId":"48cbf717-e24e-475a-9ef2-147a0b8295f5"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hello',\n"," 'welcome',\n"," 'to',\n"," 'NLP',\n"," \"'s\",\n"," 'first',\n"," 'session',\n"," '.',\n"," 'This',\n"," 'notebook',\n"," 'is',\n"," 'overview',\n"," 'of',\n"," 'Tookenization',\n"," ',',\n"," 'have',\n"," 'patience',\n"," 'you',\n"," 'will',\n"," 'excel',\n"," 'it',\n"," '.']"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["for sentence in documents:\n","  print(word_tokenize(sentence))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SgA5Y0Tm4mNn","executionInfo":{"status":"ok","timestamp":1707331615345,"user_tz":-330,"elapsed":5,"user":{"displayName":"Harsh Vardhan","userId":"09023081301295531991"}},"outputId":"7874a5fb-1147-46de-fe0d-9417ed4d7fee"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', 'welcome', 'to', 'NLP', \"'s\", 'first', 'session', '.']\n","['This', 'notebook', 'is', 'overview', 'of', 'Tookenization', ',', 'have', 'patience', 'you', 'will', 'excel', 'it', '.']\n"]}]},{"cell_type":"code","source":["#to seprate the punctuations will use some other words\n","from nltk.tokenize import wordpunct_tokenize\n","wordpunct_tokenize(corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Wkz3hpQ4woB","executionInfo":{"status":"ok","timestamp":1707331616061,"user_tz":-330,"elapsed":4,"user":{"displayName":"Harsh Vardhan","userId":"09023081301295531991"}},"outputId":"a59c0f1e-c70c-47c2-9a31-c00e44c1357e"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hello',\n"," 'welcome',\n"," 'to',\n"," 'NLP',\n"," \"'\",\n"," 's',\n"," 'first',\n"," 'session',\n"," '.',\n"," 'This',\n"," 'notebook',\n"," 'is',\n"," 'overview',\n"," 'of',\n"," 'Tookenization',\n"," ',',\n"," 'have',\n"," 'patience',\n"," 'you',\n"," 'will',\n"," 'excel',\n"," 'it',\n"," '.']"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["#full stop will not be considered as a seprate words except the last one\n","from nltk.tokenize import TreebankWordTokenizer\n","tokenizer = TreebankWordTokenizer()\n","tokenizer.tokenize(corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q4TDOxCw5GRc","executionInfo":{"status":"ok","timestamp":1707331827404,"user_tz":-330,"elapsed":22,"user":{"displayName":"Harsh Vardhan","userId":"09023081301295531991"}},"outputId":"b777c569-5bb9-43d0-844b-140566fcf4e8"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hello',\n"," 'welcome',\n"," 'to',\n"," 'NLP',\n"," \"'s\",\n"," 'first',\n"," 'session.',\n"," 'This',\n"," 'notebook',\n"," 'is',\n"," 'overview',\n"," 'of',\n"," 'Tookenization',\n"," ',',\n"," 'have',\n"," 'patience',\n"," 'you',\n"," 'will',\n"," 'excel',\n"," 'it',\n"," '.']"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":[],"metadata":{"id":"61qthbaz5ruo"},"execution_count":null,"outputs":[]}]}